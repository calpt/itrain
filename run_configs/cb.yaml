dataset:
  dataset_name: super_glue
  task_name: cb
  max_seq_length: 256
model:
  model_name_or_path: roberta-base
  use_fast_tokenizer: true
  train_adapter: true
  adapter_config: pfeiffer
training:
  learning_rate: 0.0001
  batch_size: 8
  num_train_epochs: 15
  patience: 5
  patience_metric: eval_accuracy
evaluation: true
logging:
  wandb:
    project: itrain-experiments
restarts: 3
