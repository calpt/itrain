dataset:
  dataset_name: glue
  task_name: qqp
  max_seq_length: 128
model:
  model_name_or_path: roberta-base
  use_fast_tokenizer: true
  train_adapter: true
  adapter_config: pfeiffer
training:
  learning_rate: 0.0001
  batch_size: 16
  gradient_accumulation_steps: 2
  num_train_epochs: 15
  patience: 4
  patience_metric: eval_accuracy
evaluation: true
logging:
  wandb:
    project: itrain-experiments
restarts: 3
